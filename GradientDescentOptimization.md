<h3>Coursera Gradient Descent 요약</h3><br>

cost function J(theta)를 최소화하기 위해 기울기(gradient) dJ/d(theta)를 이용하는 방법.<br>
Gradient Descent에서는 theta에 대해 gradient의 반대방향으로 일정크기만큼 이동하는 것을 반복해 J의 갑슬 최소화하는<br>
theta의 값을 찾는다.
